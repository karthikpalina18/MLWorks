{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOr2XP98ENqtsvAZiTEroc4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":19,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJ8MfncaVeOc","executionInfo":{"status":"ok","timestamp":1751899654612,"user_tz":-330,"elapsed":11196,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"ed0851bf-d95d-4591-92e9-90b616f5125f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":19}],"source":["# Install NLTK if not already installed\n","!pip install nltk\n","\n","# Import necessary modules\n","import nltk\n","\n","# Download required datasets\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["from nltk.data import find\n","\n","# This will throw an error if 'punkt' is missing\n","find('tokenizers/punkt')\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"AU19cNpJVi5d","executionInfo":{"status":"ok","timestamp":1751899654713,"user_tz":-330,"elapsed":104,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"a1f9a2e2-fbda-48be-fe41-3d26a7013bbb"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FileSystemPathPointer('/root/nltk_data/tokenizers/punkt')"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["from nltk.tokenize import word_tokenize\n","\n","text = \"NLP is a field of artificial intelligence that focuses on enabling computers to understand and generate human language.\"\n","\n","tokens = word_tokenize(text)\n","print(\"Tokens:\", tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vxNZj_K0WbwU","executionInfo":{"status":"ok","timestamp":1751899609308,"user_tz":-330,"elapsed":83,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"fc5fa49e-c7fb-4019-d693-3bf128c4594d"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokens: ['NLP', 'is', 'a', 'field', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'enabling', 'computers', 'to', 'understand', 'and', 'generate', 'human', 'language', '.']\n"]}]},{"cell_type":"code","source":["from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","\n","# Stemming Example\n","ps = PorterStemmer()\n","sentence = \"NLP is a sub entity of AI\"\n","\n","# Tokenize sentence into words and stem each word\n","for word in sentence.split():\n","    print(f\"{word} --> {ps.stem(word)}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"au-rCDTeVmMv","executionInfo":{"status":"ok","timestamp":1751899609973,"user_tz":-330,"elapsed":61,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"b1d03444-af91-4081-b928-e36a60be00aa"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["NLP --> nlp\n","is --> is\n","a --> a\n","sub --> sub\n","entity --> entiti\n","of --> of\n","AI --> ai\n"]}]},{"cell_type":"code","source":["# Lemmatization Example\n","lemmatizer = WordNetLemmatizer()\n","\n","# Examples with verbs\n","print(\"Lemmatized 'read':\", lemmatizer.lemmatize(\"read\", pos=\"v\"))\n","print(\"Lemmatized 'teaching':\", lemmatizer.lemmatize(\"teaching\", pos=\"v\"))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NqJiVfnIVoQq","executionInfo":{"status":"ok","timestamp":1751899494434,"user_tz":-330,"elapsed":24,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"61bb56e1-d370-4172-fe97-72e9d1ba4b9a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Lemmatized 'read': read\n","Lemmatized 'teaching': teach\n"]}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","\n","# Stop Words Removal Example\n","stop_words = set(stopwords.words('english'))\n","\n","sample_text = \"This is an example showing off the stop words filtration using NLTK.\"\n","\n","# Tokenize text\n","words = word_tokenize(sample_text)\n","\n","# Filter out stop words\n","filtered_sentence = [word for word in words if word.lower() not in stop_words]\n","\n","print(\"Original Words:\", words)\n","print(\"Filtered Words:\", filtered_sentence)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G_UDLoPjVrGb","executionInfo":{"status":"ok","timestamp":1751899610642,"user_tz":-330,"elapsed":68,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"99137b6f-d3e0-45e9-a2d0-6ba2c18e4d11"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Original Words: ['This', 'is', 'an', 'example', 'showing', 'off', 'the', 'stop', 'words', 'filtration', 'using', 'NLTK', '.']\n","Filtered Words: ['example', 'showing', 'stop', 'words', 'filtration', 'using', 'NLTK', '.']\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"T2dQ52E0VtFm"},"execution_count":null,"outputs":[]}]}