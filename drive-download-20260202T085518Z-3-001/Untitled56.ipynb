{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPUOZW0K4q/Km8alFq3Sfqf"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"j69OJxqJTU7n"},"outputs":[],"source":[]},{"cell_type":"markdown","metadata":{"id":"e13eb07c"},"source":["# Task\n","Implement text preprocessing using NLTK, including tokenization, stop word removal, stemming, and lemmatization."]},{"cell_type":"markdown","metadata":{"id":"07aa8e64"},"source":["## Install nltk\n","\n","### Subtask:\n","Install the NLTK library and download necessary data.\n"]},{"cell_type":"markdown","metadata":{"id":"f9dc5faf"},"source":["**Reasoning**:\n","Install NLTK and download necessary data for text preprocessing.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ba11adc","executionInfo":{"status":"ok","timestamp":1751345478295,"user_tz":-330,"elapsed":5884,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"b045ef68-1472-468d-864d-87fee8e58b7f"},"source":["pip install nltk\n"],"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n","Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.2.1)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.5.1)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"9dfc0b29"},"source":["**Reasoning**:\n","The previous command failed because `nltk.downloader.DownloadError` does not exist, and 'punkt' and 'wordnet' were not found. The corrected code will directly download 'punkt' and 'wordnet' if they are not found.\n","\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"waAJih2mT139","executionInfo":{"status":"ok","timestamp":1751345487365,"user_tz":-330,"elapsed":685,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"7e8999a4-76b1-4ff4-8981-55d56be57eab"},"source":["import nltk\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer, WordNetLemmatizer\n","\n","# Download required NLTK data (run once)\n","nltk.download('punkt')\n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","nltk.download('omw-1.4')\n","\n","# Sample input text\n","text = \"Natural Language Processing allows computers to understand human language. It's amazing, isn't it?\"\n","\n","# 1. Tokenization\n","tokens = word_tokenize(text)\n","print(\"Tokens:\", tokens)\n","\n","# 2. Removing Stop Words\n","stop_words = set(stopwords.words('english'))\n","filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n","print(\"After Stop Words Removal:\", filtered_tokens)\n","\n","# 3. Stemming\n","stemmer = PorterStemmer()\n","stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n","print(\"After Stemming:\", stemmed_tokens)\n","\n","# 4. Lemmatization\n","lemmatizer = WordNetLemmatizer()\n","lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n","print(\"After Lemmatization:\", lemmatized_tokens)\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"]},{"output_type":"stream","name":"stdout","text":["Tokens: ['Natural', 'Language', 'Processing', 'allows', 'computers', 'to', 'understand', 'human', 'language', '.', 'It', \"'s\", 'amazing', ',', 'is', \"n't\", 'it', '?']\n","After Stop Words Removal: ['Natural', 'Language', 'Processing', 'allows', 'computers', 'understand', 'human', 'language', '.', \"'s\", 'amazing', ',', \"n't\", '?']\n","After Stemming: ['natur', 'languag', 'process', 'allow', 'comput', 'understand', 'human', 'languag', '.', \"'s\", 'amaz', ',', \"n't\", '?']\n","After Lemmatization: ['Natural', 'Language', 'Processing', 'allows', 'computer', 'understand', 'human', 'language', '.', \"'s\", 'amazing', ',', \"n't\", '?']\n"]}]},{"cell_type":"markdown","metadata":{"id":"3c3b64f2"},"source":["## Tokenization\n","\n","### Subtask:\n","Split the text into individual words or tokens.\n"]},{"cell_type":"markdown","metadata":{"id":"eb20f3d6"},"source":["**Reasoning**:\n","Import the necessary function, define sample text, and perform tokenization.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"10190029"},"source":["**Reasoning**:\n","The previous command failed because the 'punkt_tab' resource was not found. I need to download it before proceeding with tokenization.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"a8cc5de7"},"source":["## Stop word removal\n","\n","### Subtask:\n","Remove common words that don't add much meaning.\n"]},{"cell_type":"markdown","metadata":{"id":"3c9b4667"},"source":["**Reasoning**:\n","Import stopwords and filter the tokens list.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"47ddb243"},"source":["## Stemming\n","\n","### Subtask:\n","Reduce words to their root form (e.g., \"running\" to \"run\").\n"]},{"cell_type":"markdown","metadata":{"id":"0ec6e354"},"source":["**Reasoning**:\n","Implement stemming to reduce words to their root form.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"bc140872"},"source":["## Lemmatization\n","\n","### Subtask:\n","Reduce words to their base or dictionary form (e.g., \"better\" to \"good\").\n"]},{"cell_type":"markdown","metadata":{"id":"55eb7e7a"},"source":["**Reasoning**:\n","Implement lemmatization using NLTK to reduce words to their base or dictionary form.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"f5246334"},"source":["**Reasoning**:\n","The previous attempt failed because the 'averaged_perceptron_tagger_eng' resource was not found. Download the required resource using nltk.download().\n","\n"]},{"cell_type":"markdown","metadata":{"id":"00b7af56"},"source":["**Reasoning**:\n","Now that the required resource is downloaded, retry the lemmatization step.\n","\n"]}]}