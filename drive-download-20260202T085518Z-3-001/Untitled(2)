{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOiyEeQmahc9JXbJYq+I/Vj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mpqcCRQEz6XS","executionInfo":{"status":"ok","timestamp":1757997528168,"user_tz":-330,"elapsed":67838,"user":{"displayName":"Palina Karthik","userId":"10747662337054099299"}},"outputId":"31f1ae9e-8a4c-49cd-a23d-42dd002fec04"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n","\u001b[1m17464789/17464789\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n","Epoch 1/2\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 50ms/step - accuracy: 0.6723 - loss: 0.5741 - val_accuracy: 0.8372 - val_loss: 0.3700\n","Epoch 2/2\n","\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 47ms/step - accuracy: 0.8812 - loss: 0.2942 - val_accuracy: 0.8386 - val_loss: 0.3576\n","\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 10ms/step - accuracy: 0.8431 - loss: 0.3605\n","\n","Test Accuracy: 84.34%\n"]}],"source":["# Install TensorFlow (includes Keras)\n","!pip install tensorflow --quiet\n","from tensorflow.keras.datasets import imdb\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","# Set vocabulary size and max sequence length\n","vocab_size = 5000\n","max_len = 100\n","# Load IMDB dataset (top 5000 words)\n","(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)\n","# Pad sequences to have same length\n","x_train = pad_sequences(x_train, maxlen=max_len)\n","x_test = pad_sequences(x_test, maxlen=max_len)\n","# Build the model\n","model = Sequential([\n","Embedding(input_dim=vocab_size, output_dim=32, input_length=max_len),\n","LSTM(32),\n","Dense(1, activation='sigmoid')\n","])\n","# Compile the model\n","model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n","# Train the model\n","history = model.fit(x_train, y_train, epochs=2, batch_size=64, validation_split=0.2)\n","# Evaluate on test set\n","loss, accuracy = model.evaluate(x_test, y_test)\n","print(f\"\\nTest Accuracy: {accuracy*100:.2f}%\")"]},{"cell_type":"code","source":[],"metadata":{"id":"3DeJosCR0E7w"},"execution_count":null,"outputs":[]}]}